site_ecoregions<-st_join(sites,EcoRegions_sf) # merge the ecoregion to dataset
site_ecoregions[is.na(site_ecoregions)] <- "COASTAL"# mutate the NA category that is sites along the coastline to Coastal for ecoregion grouping
site_ecoregions<-site_ecoregions %>%
as.data.frame() %>%
dplyr::select(SiteID,ecoregion=NA_L1NAME)
RS<-left_join(RS,site_ecoregions,by="SiteID")# Join ecoregions to dataset
# Chunk 5
get_season <- function(date_str) {
# Remove the time zone offset 'Z' from the input string
date_str <- gsub("Z$", "", date_str)
# Parse the date string using the format "%Y-%m-%dT%H:%M:%S"
date <- as.POSIXct(date_str, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
month <- month(date)
# Determine the season based on the month following water year; Fall October-December and so on
if (month %in% c(1, 2, 3)) {
return("Winter")
} else if (month %in% c(4, 5, 6)) {
return("Spring")
} else if (month %in% c(7, 8, 9)) {
return("Summer")
} else {
return("Fall")
}
}
# Chunk 6
RS <- RS %>%
mutate(season = sapply(date_unity, get_season))
# Chunk 7
site_count<-RS %>%
group_by(SiteID) %>%
summarise(n=n())# count how many observations per site there is for filtering later
RS<-left_join(RS,site_count,by="SiteID") # join to dataset
write.csv(RS, paste0(local_Path,"aquasat_processed.csv"),row.names = FALSE)
# Chunk 1
local_Path <- "/Users/danieldominguez/Documents/Code/DOC_WSC/Data/"
# Chunk 2: setup
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(dplyr)
# Chunk 3: prep
# filter data
data <- read.csv(paste0(local_Path, "aquasat_processed.csv")) %>%
#filter(!is.na(doc)) %>% # remove rows without DOC data
filter(!is.na(doc) & !is.na(secchi)) %>% # (option) filtering out rows without DOC and Secchi
filter(pwater >= 80) %>% #filter for mostly water pixels
filter(!type == "Facility") %>% #not enough data to be representative when splitting betwen quanitles
filter(n>=10) %>% # (option) filter if there is not at least x readings for a site for better training
filter(pixelCount > 0) %>% #filter erroneous pixel counts
mutate(value = doc) %>%  #mutate doc to target value
filter_all(all_vars(!is.infinite(.))) %>% # remove rows with infinites that might have resulted from band calculations
mutate(
WC = case_when(
red < green & red > blue ~ "Green",
red > green & nir > 0.01 ~ "Brown",
green < blue ~ "BG",
TRUE ~ "Green"
)
) # Create water color classification based on literature
# Chunk 4
bands <- c("red", "green","blue", "swir1", "swir2","nir",
"NR", "BR", "GR", "SR", "BG", "RG", "NG", "SG", "BN", "GN",
"RN", "SN", "BS", "GS", "RS", "NS", "RGN", "RGB", "RGS", "RBN",
"RBS", "RNS", "GBR", "GBN", "GBS", "GRN", "GRB", "GNS",
"BRG", "BRS", "BGN", "BGS", "BNS", "NRG", "NRB", "NRS",
"NGB", "NGS", "NBS", "GR2", "GN2", "B_RG", "NS_NR", "fai",
"N_S", "N_R", "NDVI", "NDWI", "NDSSI", "GNGN"
)# (option)All available bands
# Chunk 5
# select featuers
features <- c("red", "blue","GS","green",
"swir1","swir2","SG","RBS","NGS","GBS","GR2","GN2", "GRB","N_R",
"NR","N_S", "RGB","ecoregion", "type","season","WC")  #Narrower features
filtered_data <- data %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate_if(is.character,as.factor)
#This will create a "magnitude label if below a threshold low if above high for better train test split
filtered_data$mag <- ifelse(filtered_data$value <= 40, "low", "high")
# This will write the full filtered dataset which can then be split in the python files
write.csv(filtered_data,paste0(local_Path,"aquasat_full.csv"),row.names=FALSE)
# Chunk 6
random_filter_data <- function(df, column_name, condition, filter_percentage, seed ) {
# Set a random seed for reproducibility
if (!is.null(seed)) {
set.seed(seed)
}
# Create a logical vector indicating whether rows satisfy the condition
mask <- sapply(df[[column_name]], condition)
# Randomly select the indices to keep based on the filter_percentage
num_to_keep <- round(sum(mask) * (1 - filter_percentage))
indices_to_keep <- sample(which(mask), num_to_keep)
# Filter the DataFrame based on the selected indices
filtered_df <- df[indices_to_keep, ]
return(filtered_df)
}
# Chunk 7
# this is an experimental split I was using to artificially reduce the amount of low observations similar to Gardner et al., 2023
rf_data_low<-random_filter_data(filtered_data, 'value', function(x) x > 0 & x < 10, filter_percentage = 0.95, seed = 01) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate(mag="low")
rf_data_med<-random_filter_data(filtered_data, 'value', function(x) x > 10 & x < 40, filter_percentage = 0.9, seed = 01) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="low")
#We keep all the data that is a high outlier
rf_data_high<- data %>%
filter(value>=40) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="high")
rf_data<-rbind(rf_data_low,rf_data_med,rf_data_high)
#write.csv(rf_data,paste0(local_Path,"rf_aquasat_full.csv"),row.names=FALSE)
# Chunk 8
# Pre split the data into train and test for better stratification sampling
set.seed(15) # set seed for reproducibility
train_v1 <- filtered_data %>%
group_by( mag) %>%
sample_frac(.8) %>%
ungroup() %>%
arrange(value)
# Select all uniqueID's not selected in the training set
test_v1 <- filtered_data %>%
filter(!(uniqueID %in% train_v1$uniqueID)) %>%
arrange(value)
write.csv(train_v1,paste0(local_Path,"DOC_train_v1.csv"),row.names=FALSE)
write.csv(test_v1,paste0(local_Path,"DOC_test_v1.csv"),row.names=FALSE)
# Chunk 1
local_Path <- "/Users/danieldominguez/Documents/Code/DOC_WSC/Data/"
# Chunk 2: setup
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(dplyr)
# Chunk 3: prep
# filter data
data <- read.csv(paste0(local_Path, "aquasat_processed.csv")) %>%
#filter(!is.na(doc)) %>% # remove rows without DOC data
filter(!is.na(doc) & !is.na(secchi)) %>% # (option) filtering out rows without DOC and Secchi
filter(pwater >= 80) %>% #filter for mostly water pixels
filter(!type == "Facility") %>% #not enough data to be representative when splitting betwen quanitles
filter(n>=10) %>% # (option) filter if there is not at least x readings for a site for better training
filter(pixelCount > 0) %>% #filter erroneous pixel counts
mutate(value = doc) %>%  #mutate doc to target value
filter_all(all_vars(!is.infinite(.))) %>% # remove rows with infinites that might have resulted from band calculations
mutate(
WC = case_when(
red < green & red > blue ~ "Green",
red > green & nir > 0.01 ~ "Brown",
green < blue ~ "BG",
TRUE ~ "Green"
)
)%>% # Create water color classification based on literature
filter(red<2000 & red>0) %>%
filter(green<2000 &green>0) %>%
filter(blue<2000 &blue>0) %>%
filter(swir1<2000 &swir1>0) %>%
filter(swir2<2000 &swir2>0) %>%
filter(nir<2000 & nir>0)
# Chunk 4
bands <- c("red", "green","blue", "swir1", "swir2","nir",
"NR", "BR", "GR", "SR", "BG", "RG", "NG", "SG", "BN", "GN",
"RN", "SN", "BS", "GS", "RS", "NS", "RGN", "RGB", "RGS", "RBN",
"RBS", "RNS", "GBR", "GBN", "GBS", "GRN", "GRB", "GNS",
"BRG", "BRS", "BGN", "BGS", "BNS", "NRG", "NRB", "NRS",
"NGB", "NGS", "NBS", "GR2", "GN2", "B_RG", "NS_NR", "fai",
"N_S", "N_R", "NDVI", "NDWI", "NDSSI", "GNGN"
)# (option)All available bands
# Chunk 5
# select featuers
features <- c("red", "blue","GS","green",
"swir1","swir2","SG","RBS","NGS","GBS","GR2","GN2", "GRB","N_R",
"NR","N_S", "RGB","ecoregion", "type","season","WC")  #Narrower features
filtered_data <- data %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate_if(is.character,as.factor)
#This will create a "magnitude label if below a threshold low if above high for better train test split
filtered_data$mag <- ifelse(filtered_data$value <= 40, "low", "high")
# This will write the full filtered dataset which can then be split in the python files
write.csv(filtered_data,paste0(local_Path,"aquasat_full.csv"),row.names=FALSE)
# Chunk 6
random_filter_data <- function(df, column_name, condition, filter_percentage, seed ) {
# Set a random seed for reproducibility
if (!is.null(seed)) {
set.seed(seed)
}
# Create a logical vector indicating whether rows satisfy the condition
mask <- sapply(df[[column_name]], condition)
# Randomly select the indices to keep based on the filter_percentage
num_to_keep <- round(sum(mask) * (1 - filter_percentage))
indices_to_keep <- sample(which(mask), num_to_keep)
# Filter the DataFrame based on the selected indices
filtered_df <- df[indices_to_keep, ]
return(filtered_df)
}
# Chunk 7
# this is an experimental split I was using to artificially reduce the amount of low observations similar to Gardner et al., 2023
rf_data_low<-random_filter_data(filtered_data, 'value', function(x) x > 0 & x < 10, filter_percentage = 0.95, seed = 01) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate(mag="low")
rf_data_med<-random_filter_data(filtered_data, 'value', function(x) x > 10 & x < 40, filter_percentage = 0.9, seed = 01) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="low")
#We keep all the data that is a high outlier
rf_data_high<- data %>%
filter(value>=40) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="high")
rf_data<-rbind(rf_data_low,rf_data_med,rf_data_high)
#write.csv(rf_data,paste0(local_Path,"rf_aquasat_full.csv"),row.names=FALSE)
# Chunk 8
# Pre split the data into train and test for better stratification sampling
set.seed(15) # set seed for reproducibility
train_v1 <- filtered_data %>%
group_by( mag) %>%
sample_frac(.8) %>%
ungroup() %>%
arrange(value)
# Select all uniqueID's not selected in the training set
test_v1 <- filtered_data %>%
filter(!(uniqueID %in% train_v1$uniqueID)) %>%
arrange(value)
write.csv(train_v1,paste0(local_Path,"DOC_train_v1.csv"),row.names=FALSE)
write.csv(test_v1,paste0(local_Path,"DOC_test_v1.csv"),row.names=FALSE)
# Chunk 1
local_Path <- "/Users/danieldominguez/Documents/Code/DOC_WSC/Data/"
# Chunk 2: setup
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(dplyr)
# Chunk 3: prep
# filter data
data <- read.csv(paste0(local_Path, "aquasat_processed.csv")) %>%
#filter(!is.na(doc)) %>% # remove rows without DOC data
filter(!is.na(doc) & !is.na(secchi)) %>% # (option) filtering out rows without DOC and Secchi
filter(pwater >= 80) %>% #filter for mostly water pixels
filter(!type == "Facility") %>% #not enough data to be representative when splitting betwen quanitles
filter(n>=10) %>% # (option) filter if there is not at least x readings for a site for better training
filter(pixelCount > 0) %>% #filter erroneous pixel counts
mutate(value = doc) %>%  #mutate doc to target value
filter_all(all_vars(!is.infinite(.))) %>% # remove rows with infinites that might have resulted from band calculations
mutate(
WC = case_when(
red < green & red > blue ~ "Green",
red > green & nir > 0.01 ~ "Brown",
green < blue ~ "BG",
TRUE ~ "Green"
)
)%>% # Create water color classification based on literature
filter(red<2000 & red>0) %>%
filter(green<2000 &green>0) %>%
filter(blue<2000 &blue>0) %>%
filter(swir1<2000 &swir1>0) %>%
filter(swir2<2000 &swir2>0) %>%
filter(nir<2000 & nir>0)
# Chunk 4
bands <- c("red", "green","blue", "swir1", "swir2","nir",
"NR", "BR", "GR", "SR", "BG", "RG", "NG", "SG", "BN", "GN",
"RN", "SN", "BS", "GS", "RS", "NS", "RGN", "RGB", "RGS", "RBN",
"RBS", "RNS", "GBR", "GBN", "GBS", "GRN", "GRB", "GNS",
"BRG", "BRS", "BGN", "BGS", "BNS", "NRG", "NRB", "NRS",
"NGB", "NGS", "NBS", "GR2", "GN2", "B_RG", "NS_NR", "fai",
"N_S", "N_R", "NDVI", "NDWI", "NDSSI", "GNGN"
)# (option)All available bands
# Chunk 5
# select featuers
features <- c("red", "blue","GS","green",
"swir1","swir2","SG","RBS","NGS","GBS","GR2","GN2", "GRB","N_R",
"NR","N_S", "RGB","ecoregion", "type","season","WC")  #Narrower features
filtered_data <- data %>%
dplyr::select(features, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate_if(is.character,as.factor)
#This will create a "magnitude label if below a threshold low if above high for better train test split
filtered_data$mag <- ifelse(filtered_data$value <= 40, "low", "high")
# This will write the full filtered dataset which can then be split in the python files
write.csv(filtered_data,paste0(local_Path,"aquasat_full.csv"),row.names=FALSE)
# Chunk 6
random_filter_data <- function(df, column_name, condition, filter_percentage, seed ) {
# Set a random seed for reproducibility
if (!is.null(seed)) {
set.seed(seed)
}
# Create a logical vector indicating whether rows satisfy the condition
mask <- sapply(df[[column_name]], condition)
# Randomly select the indices to keep based on the filter_percentage
num_to_keep <- round(sum(mask) * (1 - filter_percentage))
indices_to_keep <- sample(which(mask), num_to_keep)
# Filter the DataFrame based on the selected indices
filtered_df <- df[indices_to_keep, ]
return(filtered_df)
}
# Chunk 7
# this is an experimental split I was using to artificially reduce the amount of low observations similar to Gardner et al., 2023
rf_data_low<-random_filter_data(filtered_data, 'value', function(x) x > 0 & x < 10, filter_percentage = 0.95, seed = 01) %>%
dplyr::select(features, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate(mag="low")
rf_data_med<-random_filter_data(filtered_data, 'value', function(x) x > 10 & x < 40, filter_percentage = 0.9, seed = 01) %>%
dplyr::select(features, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="low")
#We keep all the data that is a high outlier
rf_data_high<- data %>%
filter(value>=40) %>%
dplyr::select(features, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="high")
rf_data<-rbind(rf_data_low,rf_data_med,rf_data_high)
#write.csv(rf_data,paste0(local_Path,"rf_aquasat_full.csv"),row.names=FALSE)
# Chunk 8
# Pre split the data into train and test for better stratification sampling
set.seed(15) # set seed for reproducibility
train_v1 <- filtered_data %>%
group_by( mag) %>%
sample_frac(.8) %>%
ungroup() %>%
arrange(value)
# Select all uniqueID's not selected in the training set
test_v1 <- filtered_data %>%
filter(!(uniqueID %in% train_v1$uniqueID)) %>%
arrange(value)
write.csv(train_v1,paste0(local_Path,"DOC_train_v1.csv"),row.names=FALSE)
write.csv(test_v1,paste0(local_Path,"DOC_test_v1.csv"),row.names=FALSE)
# Chunk 1
local_Path <- "/Users/danieldominguez/Documents/Code/DOC_WSC/Data/"
# Chunk 2: setup
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(dplyr)
# Chunk 3: prep
# filter data
data <- read.csv(paste0(local_Path, "aquasat_processed.csv")) %>%
#filter(!is.na(doc)) %>% # remove rows without DOC data
filter(!is.na(doc) & !is.na(secchi)) %>% # (option) filtering out rows without DOC and Secchi
filter(pwater >= 80) %>% #filter for mostly water pixels
filter(!type == "Facility") %>% #not enough data to be representative when splitting betwen quanitles
filter(n>=10) %>% # (option) filter if there is not at least x readings for a site for better training
filter(pixelCount > 0) %>% #filter erroneous pixel counts
mutate(value = doc) %>%  #mutate doc to target value
filter_all(all_vars(!is.infinite(.))) %>% # remove rows with infinites that might have resulted from band calculations
mutate(
WC = case_when(
red < green & red > blue ~ "Green",
red > green & nir > 0.01 ~ "Brown",
green < blue ~ "BG",
TRUE ~ "Green"
)
)%>% # Create water color classification based on literature
filter(red<2000 & red>0) %>%
filter(green<2000 &green>0) %>%
filter(blue<2000 &blue>0) %>%
filter(swir1<2000 &swir1>0) %>%
filter(swir2<2000 &swir2>0) %>%
filter(nir<2000 & nir>0)
# Chunk 4
bands <- c("red", "green","blue", "swir1", "swir2","nir",
"NR", "BR", "GR", "SR", "BG", "RG", "NG", "SG", "BN", "GN",
"RN", "SN", "BS", "GS", "RS", "NS", "RGN", "RGB", "RGS", "RBN",
"RBS", "RNS", "GBR", "GBN", "GBS", "GRN", "GRB", "GNS",
"BRG", "BRS", "BGN", "BGS", "BNS", "NRG", "NRB", "NRS",
"NGB", "NGS", "NBS", "GR2", "GN2", "B_RG", "NS_NR", "fai",
"N_S", "N_R", "NDVI", "NDWI", "NDSSI", "GNGN"
)# (option)All available bands
# Chunk 5
# select featuers
features <- c("red", "blue","GS","green",
"swir1","swir2","SG","RBS","NGS","GBS","GR2","GN2", "GRB","N_R",
"NR","N_S", "RGB","ecoregion", "type","season","WC")  #Narrower features
filtered_data <- data %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate_if(is.character,as.factor)
#This will create a "magnitude label if below a threshold low if above high for better train test split
filtered_data$mag <- ifelse(filtered_data$value <= 40, "low", "high")
# This will write the full filtered dataset which can then be split in the python files
write.csv(filtered_data,paste0(local_Path,"aquasat_full.csv"),row.names=FALSE)
# Chunk 6
random_filter_data <- function(df, column_name, condition, filter_percentage, seed ) {
# Set a random seed for reproducibility
if (!is.null(seed)) {
set.seed(seed)
}
# Create a logical vector indicating whether rows satisfy the condition
mask <- sapply(df[[column_name]], condition)
# Randomly select the indices to keep based on the filter_percentage
num_to_keep <- round(sum(mask) * (1 - filter_percentage))
indices_to_keep <- sample(which(mask), num_to_keep)
# Filter the DataFrame based on the selected indices
filtered_df <- df[indices_to_keep, ]
return(filtered_df)
}
# Chunk 7
# this is an experimental split I was using to artificially reduce the amount of low observations similar to Gardner et al., 2023
rf_data_low<-random_filter_data(filtered_data, 'value', function(x) x > 0 & x < 10, filter_percentage = 0.95, seed = 01) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate(mag="low")
rf_data_med<-random_filter_data(filtered_data, 'value', function(x) x > 10 & x < 40, filter_percentage = 0.9, seed = 01) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="low")
#We keep all the data that is a high outlier
rf_data_high<- data %>%
filter(value>=40) %>%
dplyr::select(bands, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="high")
rf_data<-rbind(rf_data_low,rf_data_med,rf_data_high)
#write.csv(rf_data,paste0(local_Path,"rf_aquasat_full.csv"),row.names=FALSE)
# Chunk 8
# Pre split the data into train and test for better stratification sampling
set.seed(15) # set seed for reproducibility
train_v1 <- filtered_data %>%
group_by( mag) %>%
sample_frac(.8) %>%
ungroup() %>%
arrange(value)
# Select all uniqueID's not selected in the training set
test_v1 <- filtered_data %>%
filter(!(uniqueID %in% train_v1$uniqueID)) %>%
arrange(value)
write.csv(train_v1,paste0(local_Path,"DOC_train_v1.csv"),row.names=FALSE)
write.csv(test_v1,paste0(local_Path,"DOC_test_v1.csv"),row.names=FALSE)
# Chunk 1
local_Path <- "/Users/danieldominguez/Documents/Code/DOC_WSC/Data/"
# Chunk 2: setup
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(dplyr)
# Chunk 3: prep
# filter data
data <- read.csv(paste0(local_Path, "aquasat_processed.csv")) %>%
#filter(!is.na(doc)) %>% # remove rows without DOC data
filter(!is.na(doc) & !is.na(secchi)) %>% # (option) filtering out rows without DOC and Secchi
filter(pwater >= 80) %>% #filter for mostly water pixels
filter(!type == "Facility") %>% #not enough data to be representative when splitting betwen quanitles
filter(n>=10) %>% # (option) filter if there is not at least x readings for a site for better training
filter(pixelCount > 0) %>% #filter erroneous pixel counts
mutate(value = doc) %>%  #mutate doc to target value
filter_all(all_vars(!is.infinite(.))) %>% # remove rows with infinites that might have resulted from band calculations
mutate(
WC = case_when(
red < green & red > blue ~ "Green",
red > green & nir > 0.01 ~ "Brown",
green < blue ~ "BG",
TRUE ~ "Green"
)
)%>% # Create water color classification based on literature
filter(red<2000 & red>0) %>%
filter(green<2000 &green>0) %>%
filter(blue<2000 &blue>0) %>%
filter(swir1<2000 &swir1>0) %>%
filter(swir2<2000 &swir2>0) %>%
filter(nir<2000 & nir>0)
# Chunk 4
bands <- c("red", "green","blue", "swir1", "swir2","nir",
"NR", "BR", "GR", "SR", "BG", "RG", "NG", "SG", "BN", "GN",
"RN", "SN", "BS", "GS", "RS", "NS", "RGN", "RGB", "RGS", "RBN",
"RBS", "RNS", "GBR", "GBN", "GBS", "GRN", "GRB", "GNS",
"BRG", "BRS", "BGN", "BGS", "BNS", "NRG", "NRB", "NRS",
"NGB", "NGS", "NBS", "GR2", "GN2", "B_RG", "NS_NR", "fai",
"N_S", "N_R", "NDVI", "NDWI", "NDSSI", "GNGN"
)# (option)All available bands
# Chunk 5
# select featuers
features <- c("red", "blue","GS","green",
"swir1","swir2","SG","RBS","NGS","GBS","GR2","GN2", "GRB","N_R",
"NR","N_S", "RGB","ecoregion", "type","season","WC")  #Narrower features
filtered_data <- data %>%
dplyr::select(features, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate_if(is.character,as.factor)
#This will create a "magnitude label if below a threshold low if above high for better train test split
filtered_data$mag <- ifelse(filtered_data$value <= 40, "low", "high")
# This will write the full filtered dataset which can then be split in the python files
write.csv(filtered_data,paste0(local_Path,"aquasat_full.csv"),row.names=FALSE)
# Chunk 6
random_filter_data <- function(df, column_name, condition, filter_percentage, seed ) {
# Set a random seed for reproducibility
if (!is.null(seed)) {
set.seed(seed)
}
# Create a logical vector indicating whether rows satisfy the condition
mask <- sapply(df[[column_name]], condition)
# Randomly select the indices to keep based on the filter_percentage
num_to_keep <- round(sum(mask) * (1 - filter_percentage))
indices_to_keep <- sample(which(mask), num_to_keep)
# Filter the DataFrame based on the selected indices
filtered_df <- df[indices_to_keep, ]
return(filtered_df)
}
# Chunk 7
# this is an experimental split I was using to artificially reduce the amount of low observations similar to Gardner et al., 2023
rf_data_low<-random_filter_data(filtered_data, 'value', function(x) x > 0 & x < 10, filter_percentage = 0.95, seed = 01) %>%
dplyr::select(features, value, uniqueID,ecoregion, type,season,WC,secchi) %>%
mutate(mag="low")
rf_data_med<-random_filter_data(filtered_data, 'value', function(x) x > 10 & x < 40, filter_percentage = 0.9, seed = 01) %>%
dplyr::select(features, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="low")
#We keep all the data that is a high outlier
rf_data_high<- data %>%
filter(value>=40) %>%
dplyr::select(features, value, uniqueID,ecoregion, type,season,WC,secchi)%>%
mutate(mag="high")
rf_data<-rbind(rf_data_low,rf_data_med,rf_data_high)
#write.csv(rf_data,paste0(local_Path,"rf_aquasat_full.csv"),row.names=FALSE)
# Chunk 8
# Pre split the data into train and test for better stratification sampling
set.seed(15) # set seed for reproducibility
train_v1 <- filtered_data %>%
group_by( mag) %>%
sample_frac(.8) %>%
ungroup() %>%
arrange(value)
# Select all uniqueID's not selected in the training set
test_v1 <- filtered_data %>%
filter(!(uniqueID %in% train_v1$uniqueID)) %>%
arrange(value)
write.csv(train_v1,paste0(local_Path,"DOC_train_v1.csv"),row.names=FALSE)
write.csv(test_v1,paste0(local_Path,"DOC_test_v1.csv"),row.names=FALSE)
