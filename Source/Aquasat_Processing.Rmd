---
title: "Aquasat_Processing"
author: "Daniel Dominguez"
date: '2023-06-07'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
local_Path<- ("/Users/danieldominguez/Documents/Code/DOC_WSC/Data/") 

```


```{r setup, include=FALSE}
library(tidyverse)
library(sf)
library(dplyr)
library(mapview)
```


# Download Aquasat File

Check for presence of Aquasat matchup file - if it doesn't exist, download it.

```{r}

# url <- "https://figshare.com/ndownloader/files/18733733"
# name <- "sr_wq_rs_join.csv"
# if (file.exists(file.path(local_Path, name))) {
#   "The Aquasat file has already been downloaded."
# } else {
#   # the download will take some time.
#   options(timeout=10000)
#   download.file(url = url, 
#                 destfile = file.path(local_Path, name))
#   "The Aquasat file has been downloaded"
# }
```

# High-level QAQC

```{r}
# Filter RS data to reasonable range (0-2000), images with at least 8 pixels
RS <- read_csv(file.path(local_Path,"sr_wq_rs_join.csv")) %>% #tidy, number of pixels contributing as a cutoff
  filter(red<2000 & red>0) %>% 
  filter(green<2000 &green>0) %>% 
  filter(blue<2000 &blue>0) %>% 
  filter(swir1<2000 &swir1>0) %>% 
  filter(swir2<2000 &swir2>0) %>% 
  filter(nir<2000 & nir>0) %>% 
  mutate(NR=nir/red, #calculate the bands in JG new study
         BR=blue/red,
         GR=green/red,
         SR=swir1/red,
         BG=blue/green,
         RG=red/green,
         NG=nir/green,
         SG=swir1/green,
         BN=blue/nir,
         GN=green/nir,
         RN=red/nir,
         SN=swir1/nir,
         BS=blue/swir1,
         GS=green/swir1,
         RS=red/swir1,
         NS=nir/swir1,
         RGN=red/(green+nir),
         RGB=red/(green+blue),
         RGS=red/(green+swir1),
         RBN=red/(blue+nir),
         RBS=red/(blue+swir1),
         RNS=red/(nir+swir1),
         GBR=green/(blue+swir1),
         GBN=green/(blue+nir),
         GBS=green/(blue+swir1),
         GRN=green/(red+nir),
         GRB=green/(red+blue),
         GNS=green/(nir+swir1),
         BRG=blue/(red+green),
         BRS=blue/(red+swir1),
         BGN=blue/(green+nir),
         BGS=blue/(green+swir1),
         BNS=blue/(nir+swir1),
         NRG=nir/(red+green),
         NRB=nir/(red+blue),
         NRS=nir/(red+swir1),
         NGB=nir/(green+blue),
         NGS=nir/(green+nir),
         NBS=nir/(blue+swir1),
         GR2=(red+green)/2,
         GN2=(nir+green)/2,
         B_RG=(blue-red)/green,
         NS_NR=(nir-swir1)/(red-swir1),
         fai=nir - (red + (swir1-red)*((830-660)/(1650-660))),
         N_S=nir-swir1,
         N_R=nir-red,
         NDVI=((nir - red)/(nir + red)),
         NDWI=((green - swir1)/(green + swir1)),
         NDSSI=((blue - nir)/ (blue + nir)),
         GNGN=((green- nir)/ (green + nir))) %>% 
    mutate(uniqueID = row_number())
```

```{r}
RS<-RS %>% 
  dplyr::mutate(uniqueID = row_number())%>% # remove rows with infinites that might have resulted from band calculations
  mutate(
    WC = case_when(
      red < green & red > blue ~ "Green",
      red > green & nir > 0.01 ~ "Brown",
      green < blue ~ "BG",
      TRUE ~ "Green"
    )
  )


## Read in and convert the sites to extract the ecoregion they are located in
sites <- RS %>% 
  select(SiteID,lat, long, type) %>%  
  unique() %>% 
  st_as_sf(coords=c("long","lat"), crs=4269) 


# url <- "https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na/na_cec_eco_l1.zip"
# name <- "na_cec_eco_l1.zip"
# if (file.exists(file.path(local_Path, "ecoregions/NA_CEC_Eco_Level1.shp"))) {
#   "The ecoregion file has already been downloaded."
# } else {
#   dir.create(file.path(local_Path, "ecoregions"))
#   download.file(url = url, 
#                 destfile = file.path(local_Path, "ecoregions", name))
#   unzip(file.path(local_Path, "ecoregions", name), 
#         exdir = file.path(local_Path, "ecoregions"))
#   unlink(file.path(local_Path, "ecoregions", name))
# }


EcoRegions <- st_read(dsn=paste0(local_Path,"ecoregions/NA_CEC_Eco_Level2.shp")) %>%  #Load ecoregions
  rowid_to_column()
  
EcoRegions_sf <- EcoRegions %>% 
  st_transform(crs = 4269) %>% 
  st_make_valid() #Extract the ecoregion for each site
  
Ecoregions_buffer<-EcoRegions_sf %>% 
  st_buffer( dist = 1)

Ecoregions_rowID<-EcoRegions %>% 
  select(rowid,NA_L2NAME) %>% #change between NA_l!NAME and NA_L2NAME for level 1 or two
  st_drop_geometry() %>% 
  mutate()

mapview(EcoRegions_sf)
```

```{r}
#Extract the ecoregion for each site
site_ecoregions<-st_join(sites,EcoRegions_sf) # merge the ecoregion to dataset


Sites_remaining<-site_ecoregions %>% 
  filter(is.na(NA_L2NAME))%>% 
  dplyr::select(-NA_L2NAME)

#Retrieve the nearest ecoregion which is named rowid matching the previous rowid to column
Sites_remaining$rowid<-st_nearest_feature(Sites_remaining,EcoRegions_sf) 

Sites_remaining<-left_join(Sites_remaining,Ecoregions_rowID,by="rowid") 

# Deduplicate based on SiteID
Sites_remaining <- Sites_remaining %>%
  as.data.frame() %>% 
  distinct(SiteID, .keep_all = TRUE) %>%
  select(SiteID,ecoregion=NA_L2NAME)

site_ecoregions <- site_ecoregions %>% 
  filter(!is.na(NA_L2NAME)) %>% 
  as.data.frame() %>% 
  select(SiteID,
         ecoregion=NA_L2NAME)

site_ecoregions<-rbind(site_ecoregions,Sites_remaining) %>%  
  dplyr::select(SiteID,ecoregion) %>% 
  filter(!is.na(ecoregion)) 


RS <- left_join(RS,site_ecoregions,by="SiteID")# Join ecoregions to dataset


```

```{r}
get_season <- function(date_str) {
  # Remove the time zone offset 'Z' from the input string
  date_str <- gsub("Z$", "", date_str)
  
  # Parse the date string using the format "%Y-%m-%dT%H:%M:%S"
  date <- as.POSIXct(date_str, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
  month <- month(date)
  
  # Determine the season based on the month
  if (month %in% c(12, 1, 2)) {
    return("Winter")
  } else if (month %in% c(3, 4, 5)) {
    return("Spring")
  } else if (month %in% c(6, 7, 8)) {
    return("Summer")
  } else {
    return("Fall")
  }
}
```

```{r}
RS <- RS %>%
  mutate(season = sapply(date_unity, get_season))
```


```{r}
site_count<-RS %>% 
  group_by(SiteID) %>% 
  summarise(n=n())# count how many observations per site there is for filtering later

RS<-left_join(RS,site_count,by="SiteID") # join to dataset

write_csv(RS, file.path(local_Path,"aquasat_processed.csv"))

```

